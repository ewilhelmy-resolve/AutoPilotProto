# Simple Implementation Guide - Practical Code & Configuration

## Quick Reference: What You Actually Need to Do

### 1. Security Fixes (Day 1)

#### Fix Password Hashing
```javascript
// src/utils/auth.js - CREATE THIS FILE
const bcrypt = require('bcrypt');

const hashPassword = async (password) => {
  return await bcrypt.hash(password, 10);
};

const verifyPassword = async (password, hash) => {
  return await bcrypt.compare(password, hash);
};

module.exports = { hashPassword, verifyPassword };
```

```javascript
// Update server.js - Line 506 (signup endpoint)
const { hashPassword, verifyPassword } = require('./src/utils/auth');

// Replace:
// password, // In production, this should be hashed
// With:
password: await hashPassword(password),

// Update signin endpoint - Line 575
// Replace password comparison with:
const validPassword = await verifyPassword(password, user.password);
if (!validPassword) {
  return res.status(401).json({ message: 'Invalid credentials' });
}
```

#### Create .env File
```bash
# .env (add to .gitignore!)
NODE_ENV=production
PORT=5000

# Database - CHANGE THESE VALUES
DATABASE_URL=postgresql://user:password@localhost:5432/resolve_db

# Security - GENERATE NEW SECRETS
JWT_SECRET=generate-32-character-random-string-here
SESSION_SECRET=another-32-character-random-string-here

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# Monitoring
SENTRY_DSN=your-sentry-dsn-here

# Features
ENABLE_RATE_LIMIT=true
ENABLE_WEBHOOKS=true
MAX_UPLOAD_SIZE=52428800
```

```javascript
// Add to top of server.js
require('dotenv').config();

// Remove ALL hardcoded values like:
// process.env.DATABASE_URL = process.env.DATABASE_URL || 'postgresql://...'
// DELETE THESE LINES - they expose secrets!
```

### 2. Add Redis Sessions (Day 2)

```bash
# Install dependencies
npm install redis@3.1.2 connect-redis@5.1.0 express-session@1.17.3
```

```javascript
// src/utils/session.js - CREATE THIS FILE
const session = require('express-session');
const redis = require('redis');
const RedisStore = require('connect-redis')(session);

const redisClient = redis.createClient({
  host: process.env.REDIS_HOST || 'localhost',
  port: process.env.REDIS_PORT || 6379,
  password: process.env.REDIS_PASSWORD,
  retry_strategy: (options) => {
    if (options.error && options.error.code === 'ECONNREFUSED') {
      return new Error('Redis connection refused');
    }
    if (options.total_retry_time > 1000 * 60 * 60) {
      return new Error('Redis retry time exhausted');
    }
    if (options.attempt > 10) {
      return undefined;
    }
    return Math.min(options.attempt * 100, 3000);
  }
});

redisClient.on('error', (err) => {
  console.error('Redis error:', err);
});

const sessionConfig = {
  store: new RedisStore({ client: redisClient }),
  secret: process.env.SESSION_SECRET || 'change-this-secret',
  resave: false,
  saveUninitialized: false,
  cookie: {
    secure: process.env.NODE_ENV === 'production',
    httpOnly: true,
    maxAge: 24 * 60 * 60 * 1000, // 24 hours
    sameSite: 'strict'
  }
};

module.exports = { sessionConfig, redisClient };
```

```javascript
// Update server.js - Replace in-memory sessions
const { sessionConfig, redisClient } = require('./src/utils/session');

// Remove the global 'sessions' object and related code
// Replace with:
app.use(session(sessionConfig));

// Update authentication to use req.session
// Instead of: sessions[token] = { ... }
// Use: req.session.user = { ... }
```

### 3. Add Simple Rate Limiting (Day 3)

```bash
npm install express-rate-limit@6.7.0 rate-limit-redis@3.0.1
```

```javascript
// src/middleware/rateLimiter.js - CREATE THIS FILE
const rateLimit = require('express-rate-limit');
const RedisStore = require('rate-limit-redis');
const { redisClient } = require('../utils/session');

// General API rate limit
const apiLimiter = rateLimit({
  store: new RedisStore({
    client: redisClient,
    prefix: 'rl:api:'
  }),
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // 100 requests per window
  message: 'Too many requests, please try again later.',
  standardHeaders: true,
  legacyHeaders: false,
});

// Strict limit for auth endpoints
const authLimiter = rateLimit({
  store: new RedisStore({
    client: redisClient,
    prefix: 'rl:auth:'
  }),
  windowMs: 15 * 60 * 1000,
  max: 5, // Only 5 auth attempts per 15 minutes
  message: 'Too many authentication attempts.',
  skipSuccessfulRequests: true, // Don't count successful logins
});

// Upload limit
const uploadLimiter = rateLimit({
  store: new RedisStore({
    client: redisClient,
    prefix: 'rl:upload:'
  }),
  windowMs: 60 * 60 * 1000, // 1 hour
  max: 10, // 10 uploads per hour
  message: 'Upload limit exceeded.',
});

module.exports = { apiLimiter, authLimiter, uploadLimiter };
```

```javascript
// Add to server.js
const { apiLimiter, authLimiter, uploadLimiter } = require('./src/middleware/rateLimiter');

// Apply rate limiters
app.use('/api/', apiLimiter);
app.use('/auth/signin', authLimiter);
app.use('/auth/signup', authLimiter);
app.use('/api/documents/upload', uploadLimiter);
```

### 4. Add Security Headers (Day 3)

```bash
npm install helmet@7.0.0
```

```javascript
// Add to server.js after express setup
const helmet = require('helmet');

app.use(helmet({
  contentSecurityPolicy: {
    directives: {
      defaultSrc: ["'self'"],
      styleSrc: ["'self'", "'unsafe-inline'", "https://fonts.googleapis.com"],
      scriptSrc: ["'self'", "'unsafe-inline'"],
      fontSrc: ["'self'", "https://fonts.gstatic.com"],
      imgSrc: ["'self'", "data:", "https:"],
      connectSrc: ["'self'"],
    },
  },
  crossOriginEmbedderPolicy: false, // For compatibility
}));

// Replace the permissive CORS with:
const cors = require('cors');
app.use(cors({
  origin: process.env.ALLOWED_ORIGINS?.split(',') || ['http://localhost:3000'],
  credentials: true,
  optionsSuccessStatus: 200
}));
```

### 5. Simple Caching Layer (Day 4)

```javascript
// src/utils/cache.js - CREATE THIS FILE
const { redisClient } = require('./session');

class SimpleCache {
  constructor(ttl = 3600) {
    this.ttl = ttl; // Default 1 hour
  }

  async get(key) {
    return new Promise((resolve) => {
      redisClient.get(key, (err, data) => {
        if (err || !data) {
          resolve(null);
        } else {
          try {
            resolve(JSON.parse(data));
          } catch {
            resolve(data);
          }
        }
      });
    });
  }

  async set(key, value, ttl = this.ttl) {
    const data = typeof value === 'object' ? JSON.stringify(value) : value;
    return new Promise((resolve) => {
      redisClient.setex(key, ttl, data, (err) => {
        resolve(!err);
      });
    });
  }

  async del(key) {
    return new Promise((resolve) => {
      redisClient.del(key, (err) => {
        resolve(!err);
      });
    });
  }

  // Cache wrapper for functions
  async wrap(key, fn, ttl = this.ttl) {
    const cached = await this.get(key);
    if (cached !== null) {
      return cached;
    }
    
    const result = await fn();
    await this.set(key, result, ttl);
    return result;
  }
}

module.exports = new SimpleCache();
```

```javascript
// Example usage in routes
const cache = require('../utils/cache');

// Cache expensive queries
app.get('/api/stats/:tenantId', async (req, res) => {
  const { tenantId } = req.params;
  
  const stats = await cache.wrap(
    `stats:${tenantId}`,
    async () => {
      // Expensive database query
      return await db.getStats(tenantId);
    },
    300 // Cache for 5 minutes
  );
  
  res.json(stats);
});
```

### 6. PM2 Setup (Day 5)

```bash
npm install pm2 -g
```

```javascript
// ecosystem.config.js - CREATE THIS FILE
module.exports = {
  apps: [{
    name: 'resolve-app',
    script: './server.js',
    instances: process.env.NODE_ENV === 'production' ? 'max' : 1,
    exec_mode: 'cluster',
    autorestart: true,
    watch: false,
    max_memory_restart: '1G',
    env: {
      NODE_ENV: 'development',
      PORT: 5000
    },
    env_production: {
      NODE_ENV: 'production',
      PORT: 5000
    },
    error_file: './logs/pm2-error.log',
    out_file: './logs/pm2-out.log',
    log_file: './logs/pm2-combined.log',
    time: true,
    merge_logs: true,
    log_date_format: 'YYYY-MM-DD HH:mm:ss Z'
  }]
};
```

```bash
# PM2 commands
pm2 start ecosystem.config.js --env production
pm2 logs
pm2 monit
pm2 reload resolve-app
pm2 save
pm2 startup # Generate startup script
```

### 7. Simple Error Tracking (Day 6)

```bash
npm install @sentry/node@7.77.0
```

```javascript
// src/utils/errorTracking.js - CREATE THIS FILE
const Sentry = require('@sentry/node');

const initErrorTracking = (app) => {
  if (process.env.SENTRY_DSN) {
    Sentry.init({
      dsn: process.env.SENTRY_DSN,
      environment: process.env.NODE_ENV,
      integrations: [
        new Sentry.Integrations.Http({ tracing: true }),
        new Sentry.Integrations.Express({ app }),
      ],
      tracesSampleRate: process.env.NODE_ENV === 'production' ? 0.1 : 1.0,
      beforeSend(event) {
        // Remove sensitive data
        if (event.request) {
          delete event.request.cookies;
          delete event.request.headers?.authorization;
          delete event.request.data?.password;
        }
        return event;
      },
    });
    
    // Request handler must be first
    app.use(Sentry.Handlers.requestHandler());
    
    // TracingHandler creates a trace for every incoming request
    app.use(Sentry.Handlers.tracingHandler());
  }
};

const errorHandler = () => {
  return Sentry.Handlers.errorHandler({
    shouldHandleError(error) {
      // Capture 4xx and 5xx errors
      if (error.status >= 400) {
        return true;
      }
      return false;
    },
  });
};

module.exports = { initErrorTracking, errorHandler };
```

```javascript
// Add to server.js
const { initErrorTracking, errorHandler } = require('./src/utils/errorTracking');

// After express setup
initErrorTracking(app);

// Before server.listen (as last middleware)
app.use(errorHandler());
```

### 8. Simple Nginx Load Balancer (Day 7)

```nginx
# /etc/nginx/sites-available/resolve - CREATE THIS FILE
upstream resolve_backend {
    least_conn;
    server 127.0.0.1:5001;
    server 127.0.0.1:5002;
    server 127.0.0.1:5003;
    keepalive 32;
}

server {
    listen 80;
    server_name your-domain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name your-domain.com;
    
    ssl_certificate /etc/ssl/certs/your-cert.pem;
    ssl_certificate_key /etc/ssl/private/your-key.pem;
    
    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    
    # Gzip
    gzip on;
    gzip_types text/plain application/json application/javascript text/css;
    
    # Static files
    location /static {
        alias /var/www/resolve/public;
        expires 30d;
        add_header Cache-Control "public, immutable";
    }
    
    location /uploads {
        alias /var/www/resolve/uploads;
        expires 7d;
    }
    
    # API and app
    location / {
        proxy_pass http://resolve_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Health check endpoint
    location /health {
        access_log off;
        proxy_pass http://resolve_backend;
    }
}
```

### 9. Database Connection Pool (Day 8)

```javascript
// src/database/pool.js - CREATE THIS FILE
const { Pool } = require('pg');

// Create optimized pool
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 20, // 20 connections total
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
  // Add connection retry logic
  query_timeout: 30000,
  statement_timeout: 30000,
  idle_in_transaction_session_timeout: 60000,
});

// Handle pool errors
pool.on('error', (err, client) => {
  console.error('Unexpected database error:', err);
});

// Health check query
const checkHealth = async () => {
  try {
    const result = await pool.query('SELECT 1');
    return true;
  } catch (error) {
    console.error('Database health check failed:', error);
    return false;
  }
};

// Query wrapper with automatic retry
const query = async (text, params, retries = 2) => {
  for (let i = 0; i <= retries; i++) {
    try {
      return await pool.query(text, params);
    } catch (error) {
      if (i === retries) throw error;
      console.warn(`Query failed, retry ${i + 1}/${retries}`);
      await new Promise(resolve => setTimeout(resolve, 100 * (i + 1)));
    }
  }
};

module.exports = { pool, query, checkHealth };
```

### 10. Simple Deployment Script (Day 9)

```bash
#!/bin/bash
# deploy.sh - CREATE THIS FILE

set -e  # Exit on error

echo "ðŸš€ Starting deployment..."

# Load environment
export NODE_ENV=production
source ~/.bashrc

# Pull latest code
echo "ðŸ“¦ Pulling latest code..."
git pull origin main

# Install dependencies
echo "ðŸ“¦ Installing dependencies..."
npm ci --only=production

# Run database migrations
echo "ðŸ—ƒï¸ Running migrations..."
npm run migrate

# Build any assets if needed
if [ -f "build.js" ]; then
  echo "ðŸ”¨ Building assets..."
  npm run build
fi

# Reload PM2 with zero downtime
echo "â™»ï¸ Reloading application..."
pm2 reload ecosystem.config.js --env production

# Clear cache
echo "ðŸ§¹ Clearing cache..."
redis-cli FLUSHDB

# Health check
echo "â¤ï¸ Health check..."
sleep 5
curl -f http://localhost:5000/health || exit 1

echo "âœ… Deployment complete!"
```

```bash
# Make executable
chmod +x deploy.sh

# Run deployment
./deploy.sh
```

### 11. Simple Backup Script (Day 10)

```bash
#!/bin/bash
# backup.sh - CREATE THIS FILE

# Configuration
BACKUP_DIR="/var/backups/resolve"
DB_NAME="resolve_db"
DATE=$(date +%Y%m%d_%H%M%S)

# Create backup directory
mkdir -p $BACKUP_DIR

# Backup database
echo "Backing up database..."
pg_dump $DATABASE_URL | gzip > "$BACKUP_DIR/db_$DATE.sql.gz"

# Backup uploads
echo "Backing up uploads..."
tar -czf "$BACKUP_DIR/uploads_$DATE.tar.gz" /var/www/resolve/uploads

# Keep only last 7 days of backups
echo "Cleaning old backups..."
find $BACKUP_DIR -name "*.gz" -mtime +7 -delete

echo "Backup complete: $BACKUP_DIR"

# Optional: Upload to S3
if [ ! -z "$AWS_BACKUP_BUCKET" ]; then
  aws s3 cp "$BACKUP_DIR/db_$DATE.sql.gz" "s3://$AWS_BACKUP_BUCKET/db/"
  aws s3 cp "$BACKUP_DIR/uploads_$DATE.tar.gz" "s3://$AWS_BACKUP_BUCKET/uploads/"
fi
```

```bash
# Add to crontab for daily backups at 2 AM
crontab -e
# Add: 0 2 * * * /var/www/resolve/backup.sh
```

### 12. Production Checklist

```bash
# production-checklist.sh - CREATE THIS FILE
#!/bin/bash

echo "ðŸ” Production Readiness Checklist"
echo "================================"

# Check environment variables
echo -n "âœ“ Environment variables set: "
if [ -f .env ] && [ ! -z "$DATABASE_URL" ]; then
  echo "âœ…"
else
  echo "âŒ Missing .env or DATABASE_URL"
fi

# Check Redis
echo -n "âœ“ Redis running: "
if redis-cli ping > /dev/null 2>&1; then
  echo "âœ…"
else
  echo "âŒ Redis not responding"
fi

# Check database
echo -n "âœ“ Database accessible: "
if psql $DATABASE_URL -c "SELECT 1" > /dev/null 2>&1; then
  echo "âœ…"
else
  echo "âŒ Database not accessible"
fi

# Check PM2
echo -n "âœ“ PM2 running: "
if pm2 list | grep -q "resolve-app"; then
  echo "âœ…"
else
  echo "âŒ PM2 not running resolve-app"
fi

# Check nginx
echo -n "âœ“ Nginx configured: "
if nginx -t 2>&1 | grep -q "successful"; then
  echo "âœ…"
else
  echo "âŒ Nginx configuration error"
fi

# Check SSL certificate
echo -n "âœ“ SSL certificate valid: "
if openssl x509 -checkend 86400 -noout -in /etc/ssl/certs/your-cert.pem; then
  echo "âœ…"
else
  echo "âŒ SSL certificate expired or expiring soon"
fi

# Check disk space
echo -n "âœ“ Disk space adequate: "
DISK_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $DISK_USAGE -lt 80 ]; then
  echo "âœ… (${DISK_USAGE}% used)"
else
  echo "âŒ Disk usage high (${DISK_USAGE}%)"
fi

# Check application health
echo -n "âœ“ Application healthy: "
if curl -f http://localhost:5000/health > /dev/null 2>&1; then
  echo "âœ…"
else
  echo "âŒ Health check failed"
fi

echo "================================"
echo "Review any âŒ items before going live!"
```

---

## Final Server Setup Commands

```bash
# On each application server (3 servers total)

# 1. Install Node.js 18
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# 2. Install Redis (on dedicated Redis server)
sudo apt-get install redis-server
sudo systemctl enable redis-server

# 3. Install PostgreSQL (on database server)
sudo apt-get install postgresql postgresql-contrib
sudo systemctl enable postgresql

# 4. Install Nginx (on load balancer)
sudo apt-get install nginx
sudo systemctl enable nginx

# 5. Install PM2 globally
sudo npm install pm2 -g
pm2 startup systemd
sudo env PATH=$PATH:/usr/bin pm2 startup systemd -u $USER --hp /home/$USER

# 6. Clone and setup application
cd /var/www
git clone https://github.com/your-repo/resolve-onboarding.git
cd resolve-onboarding
npm ci --only=production
cp .env.example .env
# Edit .env with your values

# 7. Start application
pm2 start ecosystem.config.js --env production
pm2 save

# 8. Setup nginx
sudo cp nginx.conf /etc/nginx/sites-available/resolve
sudo ln -s /etc/nginx/sites-available/resolve /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx

# 9. Setup firewall
sudo ufw allow 22/tcp
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw --force enable

# 10. Setup backups
chmod +x backup.sh
(crontab -l 2>/dev/null; echo "0 2 * * * /var/www/resolve-onboarding/backup.sh") | crontab -
```

---

## That's It! You're Production Ready

### What You Now Have:
âœ… Secure password hashing  
âœ… Environment-based configuration  
âœ… Redis sessions and caching  
âœ… Rate limiting on all endpoints  
âœ… Security headers (helmet)  
âœ… PM2 process management with clustering  
âœ… Nginx load balancing  
âœ… Error tracking with Sentry  
âœ… Automated backups  
âœ… Zero-downtime deployments  

### Total Time: 2 Weeks
### Total Cost: ~$135/month
### Supports: 100+ tenants easily

### Next Steps Only When Needed:
- Add CDN when static traffic increases
- Add read replicas when database CPU > 80%
- Add more app servers when needed (just clone setup)
- Consider managed services only when team is overwhelmed

**Remember:** This simple setup will handle 100 tenants better than a complex, over-engineered solution that your team doesn't understand. Keep it simple, keep it running, keep it profitable.